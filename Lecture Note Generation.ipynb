{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74062a2f-9363-44c0-a8a7-39302c66b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manna\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os  # Provides functions for interacting with the operating system\n",
    "import numpy as np  # Used for numerical operations and handling arrays\n",
    "import sounddevice as sd  # Library for recording and playing sound\n",
    "import torch  # PyTorch, used for deep learning and tensor computations\n",
    "import whisper  # OpenAI's Whisper model for speech-to-text transcription\n",
    "import wave  # Module for reading and writing WAV files\n",
    "import keyboard  # Library for detecting keyboard input events\n",
    "import threading  # Used for running multiple operations concurrently\n",
    "import noisereduce as nr  # Library for reducing noise in audio signals\n",
    "import scipy.io.wavfile as wav  # Module for handling WAV files using SciPy\n",
    "import cv2  # OpenCV for image processing tasks\n",
    "from pdf2image import convert_from_path  # Converts PDF pages to images\n",
    "import re  # Regular expressions for text processing\n",
    "import tkinter as tk  # GUI library for creating graphical interfaces\n",
    "from tkinter import filedialog  # Module for opening file dialog boxes\n",
    "import tempfile  # Library for creating temporary files and directories\n",
    "import pytesseract  # Optical Character Recognition (OCR) using Tesseract\n",
    "import gc  # Garbage collector for managing memory\n",
    "from concurrent.futures import ThreadPoolExecutor  # For parallel execution of tasks\n",
    "from nltk.tokenize import sent_tokenize  # Tokenizes text into sentences\n",
    "from nltk.corpus import wordnet  # WordNet for finding synonyms and meanings\n",
    "from sentence_transformers import SentenceTransformer, util  # Pre-trained transformer model for sentence embeddings and similarity\n",
    "import warnings  # Handles warnings in the program\n",
    "warnings.filterwarnings(\"ignore\")  # Suppresses warnings to keep the output clean\n",
    "import time  # Provides time-related functions (e.g., measuring execution time)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a8f7b2-8b1c-4740-a4c6-abfce049eef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear all unused tensors from GPU memory to free up space\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection to remove unused objects and free memory\n",
    "gc.collect()\n",
    "\n",
    "# Check if the Whisper model exists in the global namespace\n",
    "if \"whisper_model\" in globals():\n",
    "    del whisper_model  # Delete the Whisper model to free up memory\n",
    "\n",
    "# Perform another round of GPU memory clearance\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run garbage collection again to ensure all unused objects are removed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c741da-a285-4db5-9836-504cf4873945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Whisper model...\n",
      "Loaded Successfully...\n"
     ]
    }
   ],
   "source": [
    "# Function to load the Whisper model\n",
    "def load_whisper_model():\n",
    "    \"\"\"\n",
    "    Loads the Whisper model from a local .pt file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if a GPU is available; otherwise, default to CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")  # Display the selected device\n",
    "\n",
    "    # Check if the Whisper model file exists in the current directory\n",
    "    if os.path.exists(\"large-v3-001.pt\"):\n",
    "        print(\"Loading Whisper model...\")  # Notify that model loading is in progress\n",
    "        \n",
    "        # Load the Whisper model and assign it to the specified device (GPU or CPU)\n",
    "        model = whisper.load_model(\"large-v3-001.pt\", device=device)\n",
    "        \n",
    "        return model  # Return the loaded model\n",
    "    else:\n",
    "        # Raise an error if the model file is not found\n",
    "        raise FileNotFoundError(\"Whisper model not found.\")\n",
    "\n",
    "# Load the Whisper model and store it in the variable `whisper_model`\n",
    "whisper_model = load_whisper_model()\n",
    "\n",
    "# Confirm that the model has been loaded successfully\n",
    "print(\"Loaded Successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c45bac5-39ef-4761-830b-94c78627dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model for generating sentence embeddings\n",
    "mpnetMODEL = SentenceTransformer(\"paraphrase_mpnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2f2981-7213-471d-9fa4-1b36dea74c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned T5 explanation generation model and tokenizer\n",
    "model_path = \"t5_explanation_model\"\n",
    "t5model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "t5tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1307f889-c8d7-4f9a-8952-a0f149629f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned T5 summarization model and tokenizer\n",
    "model_path = \"t5_summary_model\"\n",
    "t5_summarization_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "t5_summarization_tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ff68bf-30f2-4264-9f7f-c1c4cdb27f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Flag to track whether recording is in progress\n",
    "recording = False  \n",
    "\n",
    "# List to store recorded audio data for each slide\n",
    "audio_data = []  \n",
    "\n",
    "# List to store extracted text from slides\n",
    "slide_texts = []  \n",
    "\n",
    "# Variable to keep track of the current slide index\n",
    "current_slide = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd63364-9473-4e77-bfb1-8813d66dff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open a file dialog and allow the user to select a PDF file\n",
    "def select_pdf():\n",
    "    # Create a hidden root window (Tkinter main window)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main GUI window\n",
    "    root.attributes('-topmost', True)  # Ensure the dialog appears on top\n",
    "    root.update()  # Force the window to refresh and appear correctly\n",
    "\n",
    "    # Open a file dialog to select a PDF file\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select PDF File\",  # Title of the file dialog window\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\")]  # Only allow selection of PDF files\n",
    "    )\n",
    "    \n",
    "    # Return the selected file path if a file is chosen, otherwise return None\n",
    "    return file_path if file_path else None\n",
    "\n",
    "# Function to extract slides (images) from the selected PDF\n",
    "def extract_slides():\n",
    "    # Prompt user to select a PDF file\n",
    "    pdf_path = select_pdf()\n",
    "    \n",
    "    # Check if no file was selected\n",
    "    if not pdf_path:\n",
    "        print(\"No file selected. Exiting...\")\n",
    "        exit()  # Terminate the program if no file is selected\n",
    "    \n",
    "    print(f\"Processing PDF: {pdf_path}\")  # Print selected PDF file path\n",
    "    \n",
    "    # Convert the selected PDF file into images (one per page/slide)\n",
    "    return convert_from_path(pdf_path)  # Ensure convert_from_path is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b484fe-4b97-44ff-befa-792cbeb80713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ocr_text(text):\n",
    "    \"\"\"Fixes common OCR mistakes while preserving numbering.\"\"\"\n",
    "\n",
    "    # Ensure proper spacing after multi-level numbers (e.g., \"1.1.Subheading\" → \"1.1. Subheading\")\n",
    "    text = re.sub(r\"(\\d+\\.\\d+)\\.(\\w)\", r\"\\1. \\2\", text)  \n",
    "\n",
    "    # Ensure spacing after single-level numbers (e.g., \"1.Heading\" → \"1. Heading\")\n",
    "    text = re.sub(r\"(\\d+)\\.([A-Za-z])\", r\"\\1. \\2\", text)\n",
    "\n",
    "    # Remove incorrect spaces before punctuation (e.g., \"Hello , world .\" → \"Hello, world.\")\n",
    "    text = re.sub(r\"\\s+\\.\", \".\", text)  # Remove extra spaces before a period\n",
    "    text = re.sub(r\"\\s+,\", \",\", text)  # Remove extra spaces before a comma\n",
    "\n",
    "    # Fix incorrect newlines breaking words or sentences\n",
    "    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)  # Fix hyphenated word splits across lines\n",
    "    text = re.sub(r\"(\\w)\\n(\\w)\", r\"\\1 \\2\", text)  # Merge words split by a newline into a single sentence\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba307626-a07f-499c-83b9-ecb4b4835591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_slide(slide_image, current_slide):\n",
    "    \"\"\"\n",
    "    Extracts and formats text from a slide image using OCR and text processing techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Extracting text from Slide {current_slide}...\")\n",
    "\n",
    "    # Save slide as a temporary image file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as temp_file:\n",
    "        slide_image.save(temp_file.name, format=\"PNG\")  # Save the image in PNG format\n",
    "\n",
    "    # Load the saved image and convert it to grayscale for better OCR accuracy\n",
    "    image = cv2.imread(temp_file.name)  # Read the image using OpenCV\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "\n",
    "    # Perform OCR (Optical Character Recognition) to extract text from the grayscale image\n",
    "    ocr_result = pytesseract.image_to_string(gray)  # Extract text using Tesseract OCR\n",
    "    ocr_result = clean_ocr_text(ocr_result)  # Clean the extracted text using OCR correction function\n",
    "\n",
    "    # Initialize a dictionary to store structured extracted data\n",
    "    extracted_data = {\"headings\": []}  # List to hold structured heading and text data\n",
    "    current_heading = None  # Variable to store the current heading\n",
    "    current_subheading = None  # Variable to store the current subheading\n",
    "    current_text = []  # List to store extracted text blocks\n",
    "\n",
    "    # Helper function to store collected text under the appropriate heading/subheading\n",
    "    def store_text():\n",
    "        \"\"\"\n",
    "        Stores collected text under the relevant heading or subheading.\n",
    "        \"\"\"\n",
    "        nonlocal current_heading, current_subheading, current_text\n",
    "\n",
    "        if not current_text:\n",
    "            return  # No text to store, exit function\n",
    "\n",
    "        text_block = \" \".join(current_text).strip()  # Join text lines into a single block\n",
    "\n",
    "        if current_subheading:  \n",
    "            # If a subheading exists, append text under the last subheading\n",
    "            if extracted_data[\"headings\"] and extracted_data[\"headings\"][-1][\"subheadings\"]:\n",
    "                extracted_data[\"headings\"][-1][\"subheadings\"][-1][\"text\"] += \" \" + text_block\n",
    "        elif current_heading:  \n",
    "            # If a heading exists, append text under the last heading\n",
    "            if extracted_data[\"headings\"]:\n",
    "                extracted_data[\"headings\"][-1][\"text\"] += \" \" + text_block\n",
    "\n",
    "        current_text = []  # Reset text collection for the next block\n",
    "\n",
    "    # Process extracted text line by line\n",
    "    for line in ocr_result.split(\"\\n\"):\n",
    "        line = line.strip()  # Remove leading and trailing whitespace\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Identify headings (e.g., \"1. Heading:\")\n",
    "        heading_match = re.match(r\"^(\\d+\\.)\\s(.+):?$\", line)  # Matches \"1. Heading:\"\n",
    "        subheading_match = re.match(r\"^(\\d+\\.\\d+)\\.\\s(.+):?$\", line)  # Matches \"1.1. Subheading:\"\n",
    "\n",
    "        if heading_match:\n",
    "            store_text()  # Store previous text before starting a new heading\n",
    "            current_heading = f\"{heading_match.group(1)} {heading_match.group(2)}\"  # Preserve number + heading\n",
    "            current_subheading = None  # Reset subheading\n",
    "            extracted_data[\"headings\"].append({\"heading\": current_heading, \"text\": \"\", \"subheadings\": []})  # Add heading entry\n",
    "\n",
    "        elif subheading_match:\n",
    "            store_text()  # Store previous text before starting a new subheading\n",
    "            current_subheading = f\"{subheading_match.group(1)}. {subheading_match.group(2)}\"  # Preserve correct format\n",
    "            if extracted_data[\"headings\"]:\n",
    "                extracted_data[\"headings\"][-1][\"subheadings\"].append({\"subheading\": current_subheading, \"text\": \"\"})  # Add subheading entry\n",
    "\n",
    "        else:\n",
    "            current_text.append(line)  # Collect text content\n",
    "            if line.endswith(\".\"):  # If the line ends with a period, store the text\n",
    "                store_text()\n",
    "\n",
    "    # Store any remaining text at the end of the loop\n",
    "    store_text()\n",
    "\n",
    "    return extracted_data  # Return structured extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d85c3216-9340-4896-a4fa-f66735aa773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(sample_rate=16000, stop_event=None):\n",
    "    \"\"\"\n",
    "    Records audio in a separate thread until 'SPACE' is pressed.\n",
    "\n",
    "    Parameters:\n",
    "    sample_rate (int): The sampling rate for audio recording (default: 16000 Hz).\n",
    "    stop_event (threading.Event): A threading event used to stop the recording.\n",
    "    \"\"\"    \n",
    "    global audio_data\n",
    "    audio_data = []  # Initialize an empty list to store recorded audio chunks.\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        \"\"\"\n",
    "        Callback function that gets called for each audio block captured.\n",
    "\n",
    "        Parameters:\n",
    "        indata (numpy.ndarray): The recorded audio data.\n",
    "        frames (int): Number of frames in the audio buffer.\n",
    "        time (CData): Time information for the audio.\n",
    "        status (CallbackFlags): Status flags indicating errors or warnings.\n",
    "        \"\"\"\n",
    "        if status:\n",
    "            print(status)  # Print any errors or warnings from the audio stream.\n",
    "        audio_data.append(indata.copy())  # Append the recorded audio data to the list.\n",
    "\n",
    "    # Start an audio input stream with the specified sample rate and callback function.\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, dtype=np.float32, callback=callback):\n",
    "        while not stop_event.is_set():  # Keep recording until the stop event is triggered.\n",
    "            pass  # Do nothing, just keep the loop running.\n",
    "\n",
    "    print(\"Recording stopped.\")  # Print confirmation when recording stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08dc0774-d61e-47ed-9ca1-056d5915a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Normalizes the volume and reduces background noise in the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    audio_data (numpy.ndarray): The raw audio signal as a NumPy array.\n",
    "    sample_rate (int): The sample rate of the audio (default: 16000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The processed audio signal with reduced noise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the audio to ensure values are between -1 and 1\n",
    "    audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "\n",
    "    # Reduce background noise using the \"noisereduce\" library\n",
    "    return nr.reduce_noise(y=audio_data.flatten(), sr=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2406e3e-c289-40ae-b7b4-051e165cfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(audio_filename, audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Saves the processed audio as a WAV file.\n",
    "\n",
    "    Parameters:\n",
    "    audio_filename (str): The name of the output audio file (e.g., \"lecture1.wav\").\n",
    "    audio_data (numpy.ndarray): The processed audio data as a NumPy array.\n",
    "    sample_rate (int): The sample rate of the audio (default: 16000 Hz).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the full file path where the audio will be saved\n",
    "    file_path = rf\"D:\\Studies Material\\OLA Project\\audio_files\\{audio_filename}\"\n",
    "\n",
    "    # Convert floating-point audio data (-1 to 1) to 16-bit PCM format (range: -32768 to 32767)\n",
    "    int_audio_data = (audio_data * 32767).astype(np.int16)\n",
    "\n",
    "    # Save the audio as a WAV file using scipy's wavfile.write\n",
    "    wav.write(file_path, sample_rate, int_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b7cf1a8-ceab-4ec8-abd1-b81996e96340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(whisper_model, file_path):\n",
    "    \"\"\"\n",
    "    Transcribes speech from an audio file using the Whisper model.\n",
    "\n",
    "    Parameters:\n",
    "    whisper_model (whisper.Whisper): The preloaded Whisper model used for transcription.\n",
    "    file_path (str): The path to the audio file to be transcribed.\n",
    "\n",
    "    Returns:\n",
    "    str: The transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the Whisper model to transcribe the given audio file\n",
    "    result = whisper_model.transcribe(file_path)\n",
    "\n",
    "    # Extract and return only the transcribed text from the result\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a94764dd-8085-4442-8f64-10bbce765483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from a heading by removing numbering\n",
    "def extract_keyword(title):\n",
    "    \"\"\"\n",
    "    Extracts the main keyword from a heading by removing numbering before a colon.\n",
    "    \n",
    "    Example:\n",
    "        Input: \"1.2. Introduction:\"\n",
    "        Output: \"Introduction\"\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\d+\\.\\d*\\.?\\s*(.*?):\", title)  # Extract text before \":\"\n",
    "    return match.group(1).strip() if match else title.strip()\n",
    "\n",
    "# Function to find synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Retrieves a list of synonyms for a given word using WordNet.\n",
    "    \n",
    "    Example:\n",
    "        Input: \"happy\"\n",
    "        Output: [\"glad\", \"joyful\", \"pleased\", ...]\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):  # Get all synsets of the word\n",
    "        for lemma in syn.lemmas():  # Iterate over lemmas (different word forms)\n",
    "            synonyms.add(lemma.name().lower().replace(\"_\", \" \"))  # Convert to readable format\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to generate abbreviations for a given term\n",
    "def generate_abbreviation(term):\n",
    "    \"\"\"\n",
    "    Generates an abbreviation by taking the first letter of each word in the term.\n",
    "    Common stopwords are ignored.\n",
    "\n",
    "    Example:\n",
    "        Input: \"Natural Language Processing\"\n",
    "        Output: [\"NLP\"]\n",
    "    \"\"\"\n",
    "    stopwords = {\"of\", \"and\", \"the\", \"in\", \"for\", \"on\", \"at\", \"to\", \"with\", \"by\", \"an\", \"a\"}\n",
    "    words = re.split(r'[\\s\\-]', term)  # Split words by spaces and hyphens\n",
    "    words = [w for w in words if w.isalpha()]  # Keep only alphabetic words\n",
    "\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    # Extract uppercase first letters for abbreviation (if present)\n",
    "    abbreviation = \"\".join([w[0] for w in words if w.isupper()])\n",
    "    \n",
    "    # If no uppercase abbreviation found, use all first letters except stopwords\n",
    "    if not abbreviation:\n",
    "        abbreviation = \"\".join([w[0].upper() for w in words if w.lower() not in stopwords])\n",
    "\n",
    "    return [abbreviation] if len(abbreviation) >= 2 else []\n",
    "\n",
    "# Class to manage section headings and associated text\n",
    "class SectionManager:\n",
    "    def __init__(self, heading_data, model, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes a section manager that processes headings and subheadings from a structured data format.\n",
    "\n",
    "        Parameters:\n",
    "        heading_data (dict): Contains \"headings\" with their respective texts and subheadings.\n",
    "        model (T5ForConditionalGeneration): Fine-tuned T5 model.\n",
    "        tokenizer (T5Tokenizer): Tokenizer for T5 model.\n",
    "        \"\"\"\n",
    "        self.section_labels = []  # List of section names\n",
    "        self.labels_keywords = []  # List of synonyms and abbreviations for each section\n",
    "        self.section_texts = {}  # Dictionary to store section texts\n",
    "        self.section_embeddings = {}  # Dictionary to store section embeddings\n",
    "        self.model = model  # Store model\n",
    "        self.tokenizer = tokenizer  # Store tokenizer\n",
    "\n",
    "        # Process each heading in the document\n",
    "        for heading_info in heading_data[\"headings\"]:\n",
    "            heading = extract_keyword(heading_info[\"heading\"])  # Extract cleaned heading\n",
    "            explanation = self.generate_explanation(heading)  # Generate explanation\n",
    "            combined_text = heading_info[\"text\"] + \" \" + explanation  # Combine explanation with slide text\n",
    "            self.add_section(heading, combined_text)  # Add to section manager\n",
    "\n",
    "            # Process each subheading under the current heading\n",
    "            for subheading_info in heading_info[\"subheadings\"]:\n",
    "                subheading = extract_keyword(subheading_info[\"subheading\"])\n",
    "                explanation = self.generate_explanation(subheading)  # Generate explanation\n",
    "                combined_text = subheading_info[\"text\"] + \" \" + explanation  # Combine explanation with slide text\n",
    "                self.add_section(subheading, combined_text)  # Add to section manager\n",
    "\n",
    "    def generate_explanation(self, text, max_length=120):\n",
    "        \"\"\"\n",
    "        Uses the fine-tuned T5 model to generate an explanation for a given text.\n",
    "        Cleans the explanation by removing prefixes and ensuring complete sentences.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(\"explain: \" + text, return_tensors=\"pt\").input_ids\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(input_ids, max_length=max_length, num_beams=8, no_repeat_ngram_size=3, repetition_penalty=2.0)\n",
    "        \n",
    "        explanation = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove unnecessary phrases like \"explanation of this is:\"\n",
    "        explanation = self.clean_explanation(explanation)\n",
    "        \n",
    "        # Ensure no incomplete sentences at the end\n",
    "        explanation = self.remove_incomplete_sentence(explanation)\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "    def clean_explanation(self, explanation):\n",
    "        \"\"\"\n",
    "        Removes unnecessary prefixes like 'Generated Explanation:' and handles specific \n",
    "        cases like headings followed by colons (e.g., \"Operating Systems:\").\n",
    "        \"\"\"\n",
    "        # Remove unwanted prefix\n",
    "        unwanted_prefixes = [\"generated explanation:\", \"explanation:\", \"explanation of:\"]\n",
    "        for prefix in unwanted_prefixes:\n",
    "            if explanation.lower().startswith(prefix):\n",
    "                explanation = explanation[len(prefix):].strip()\n",
    "\n",
    "        # Remove heading if it is followed by a colon (e.g., \"Operating Systems:\")\n",
    "        # Remove the part before and including the first colon\n",
    "        explanation = re.sub(r'^[^:]+:', '', explanation).strip()\n",
    "\n",
    "        return explanation.strip()\n",
    "\n",
    "    def remove_incomplete_sentence(self, explanation):\n",
    "        \"\"\"\n",
    "        Ensures that the explanation does not end with an incomplete sentence.\n",
    "        It checks if the last sentence is incomplete (i.e., does not end with a punctuation mark).\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?]) +', explanation)  # Split based on punctuation marks\n",
    "    \n",
    "        if sentences and sentences[-1] != \"\":\n",
    "            last_sentence = sentences[-1]\n",
    "        \n",
    "            # Check if the last sentence ends with a full stop or similar punctuation\n",
    "            if last_sentence[-1] not in \".!?\":\n",
    "                sentences = sentences[:-1]  # Remove the last sentence if it is incomplete\n",
    "\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    def add_section(self, label, text):\n",
    "        \"\"\"\n",
    "        Adds a new section along with its synonyms, abbreviations, and initial text.\n",
    "        Also computes an embedding for the section based on its keywords and text.\n",
    "\n",
    "        Parameters:\n",
    "        label (str): The title of the section.\n",
    "        text (str): The initial text content for the section.\n",
    "        \"\"\"\n",
    "        self.section_labels.append(label)  # Store the section label\n",
    "\n",
    "        # Generate synonyms and abbreviations for better keyword matching\n",
    "        synonyms = get_synonyms(label)\n",
    "        keyword_variants = [label] + synonyms + generate_abbreviation(label)\n",
    "        self.labels_keywords.append(keyword_variants)\n",
    "\n",
    "        # Store initial text for the section\n",
    "        self.section_texts[label] = text\n",
    "\n",
    "        # Compute embedding using SentenceTransformer\n",
    "        expanded_text = \" \".join(keyword_variants) + \" \" + text  # Combine keywords and text\n",
    "        self.section_embeddings[label] = mpnetMODEL.encode(expanded_text, convert_to_tensor=True)\n",
    "\n",
    "    def update_section(self, label, new_sentence):\n",
    "        \"\"\"\n",
    "        Updates a section by appending a new sentence and recomputing its embedding.\n",
    "\n",
    "        Parameters:\n",
    "        label (str): The section label to update.\n",
    "        new_sentence (str): The new sentence to add to the section.\n",
    "        \"\"\"\n",
    "        if label in self.section_texts:\n",
    "            self.section_texts[label] += \" \" + new_sentence  # Append new sentence\n",
    "            updated_text = \" \".join(self.labels_keywords[self.section_labels.index(label)]) + \" \" + self.section_texts[label]\n",
    "            self.section_embeddings[label] = mpnetMODEL.encode(updated_text, convert_to_tensor=True)  # Update embedding\n",
    "\n",
    "# Function to find the most relevant section for each transcribed sentence\n",
    "def find_relevant_sentences(transcribed_sentences, section_manager):\n",
    "    \"\"\"\n",
    "    Matches each transcribed sentence to the most relevant section using cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    transcribed_sentences (list): List of sentences from transcribed audio.\n",
    "    section_manager (SectionManager): The section manager containing precomputed section embeddings.\n",
    "\n",
    "    Returns:\n",
    "    dict: A mapping of section labels to their assigned transcribed sentences.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = mpnetMODEL.encode(transcribed_sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Convert section embeddings into a tensor list\n",
    "    section_labels = section_manager.section_labels\n",
    "    section_embeddings = list(section_manager.section_embeddings.values())\n",
    "\n",
    "    # If no sections exist, return empty mappings\n",
    "    if len(section_embeddings) == 0:\n",
    "        return {label: [] for label in section_labels}\n",
    "\n",
    "    section_embeddings = util.torch.stack(section_embeddings)  # Stack embeddings into tensor format\n",
    "\n",
    "    # Compute cosine similarity between each sentence and section\n",
    "    similarities = util.cos_sim(sentence_embeddings, section_embeddings)\n",
    "\n",
    "    sentence_assignments = {label: [] for label in section_labels}\n",
    "\n",
    "    # Assign each sentence to the section with the highest similarity score\n",
    "    for i, sentence in enumerate(transcribed_sentences):\n",
    "        max_index = similarities[i].argmax().item()  # Get the index of the best-matching section\n",
    "        best_match = section_labels[max_index]  # Retrieve the best-matching section label\n",
    "\n",
    "        # Assign sentence to the identified section and update section text\n",
    "        sentence_assignments[best_match].append(sentence)\n",
    "        section_manager.update_section(best_match, sentence)\n",
    "\n",
    "    return sentence_assignments\n",
    "\n",
    "# Function to merge slide text with transcribed speech\n",
    "def combine_slide_text_with_audio(slide_content, transcribed_text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Integrates extracted slide text with transcribed speech by mapping sentences to relevant sections.\n",
    "    And add explanation to the content.\n",
    "    \n",
    "    Parameters:\n",
    "    slide_content (dict): A structured dictionary containing headings and their respective texts.\n",
    "    transcribed_text (str): The transcribed audio content.\n",
    "    model (T5ForConditionalGeneration): Fine-tuned T5 model.\n",
    "    tokenizer (T5Tokenizer): Tokenizer for T5 model.\n",
    "\n",
    "    Returns:\n",
    "    dict: A structured dictionary containing headings with updated text from audio.\n",
    "    \"\"\"\n",
    "    # Tokenize transcribed text into sentences\n",
    "    transcribed_sentences = [s.strip() for s in sent_tokenize(transcribed_text) if s.strip()]\n",
    "\n",
    "    # Initialize SectionManager with the model and tokenizer\n",
    "    section_manager = SectionManager(slide_content, model, tokenizer)\n",
    "\n",
    "    # Assign transcribed sentences to the most relevant sections\n",
    "    sentence_assignments = find_relevant_sentences(transcribed_sentences, section_manager)\n",
    "\n",
    "    # Construct the final combined output structure\n",
    "    combined_output = {\"headings\": []}\n",
    "\n",
    "    for heading_data in slide_content[\"headings\"]:\n",
    "        heading = heading_data[\"heading\"]\n",
    "        heading_keyword = extract_keyword(heading)\n",
    "\n",
    "        heading_entry = {\n",
    "            \"heading\": heading,\n",
    "            \"text\": section_manager.section_texts.get(heading_keyword, \"\"),\n",
    "            \"subheadings\": []\n",
    "        }\n",
    "\n",
    "        for subheading_data in heading_data[\"subheadings\"]:\n",
    "            subheading = subheading_data[\"subheading\"]\n",
    "            subheading_keyword = extract_keyword(subheading)\n",
    "\n",
    "            subheading_entry = {\n",
    "                \"subheading\": subheading,\n",
    "                \"text\": section_manager.section_texts.get(subheading_keyword, \"\")\n",
    "            }\n",
    "\n",
    "            heading_entry[\"subheadings\"].append(subheading_entry)\n",
    "\n",
    "        combined_output[\"headings\"].append(heading_entry)\n",
    "\n",
    "    return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc93ec92-d57b-403a-871a-79dd604a583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summary\n",
    "def generate_summary(explanation_text, max_input_length=512, max_output_length=128):\n",
    "    # Prepare the input (you can prepend \"summarize: \" if your model was trained that way)\n",
    "    input_text = \"summarize: \" + explanation_text\n",
    "    input_ids = t5_summarization_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = t5_summarization_model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_output_length,\n",
    "        num_beams=8,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0\n",
    "    )\n",
    "\n",
    "    # Decode summary\n",
    "    summary = t5_summarization_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary = clean_summary_text(summary)\n",
    "    return summary\n",
    "\n",
    "# Cleaning function\n",
    "def clean_summary_text(summary):\n",
    "    \"\"\"\n",
    "    Cleans the generated summary:\n",
    "    - Removes unwanted prefixes like \"In summary,\"\n",
    "    - Ensures the first letter is capitalized\n",
    "    - Removes any incomplete sentence at the end\n",
    "    \"\"\"\n",
    "    # Remove unwanted prefix\n",
    "    unwanted_prefixes = [\"in summary,\", \"in conclusion,\", \"in short,\", \"summary:\", \"conclusion:\"]\n",
    "    for prefix in unwanted_prefixes:\n",
    "        if summary.lower().startswith(prefix):\n",
    "            summary = summary[len(prefix):].strip()\n",
    "            if summary:\n",
    "                summary = summary[0].upper() + summary[1:]\n",
    "\n",
    "    # Remove incomplete sentence at the end\n",
    "    sentences = re.split(r'(?<=[.!?]) +', summary)\n",
    "    if sentences and sentences[-1] != \"\":\n",
    "        last_sentence = sentences[-1]\n",
    "        if last_sentence[-1] not in \".!?\":\n",
    "            sentences = sentences[:-1]\n",
    "    cleaned_summary = ' '.join(sentences)\n",
    "\n",
    "    return cleaned_summary\n",
    "\n",
    "# Generating final notes\n",
    "def generate_notes(content):\n",
    "    final_notes = {\"headings\": []}\n",
    "\n",
    "    for heading_data in content[\"headings\"]:\n",
    "        heading = heading_data[\"heading\"]\n",
    "\n",
    "        # Get and summarize heading text\n",
    "        heading_text = heading_data[\"text\"]\n",
    "        summarized_heading_text = generate_summary(heading_text)\n",
    "\n",
    "        heading_entry = {\n",
    "            \"heading\": heading,\n",
    "            \"text\": summarized_heading_text,\n",
    "            \"subheadings\": []\n",
    "        }\n",
    "\n",
    "        for subheading_data in heading_data[\"subheadings\"]:\n",
    "            subheading = subheading_data[\"subheading\"]\n",
    "\n",
    "            sub_text = subheading_data[\"text\"]\n",
    "            summarized_sub_text = generate_summary(sub_text)\n",
    "\n",
    "            subheading_entry = {\n",
    "                \"subheading\": subheading,\n",
    "                \"text\": summarized_sub_text\n",
    "            }\n",
    "\n",
    "            heading_entry[\"subheadings\"].append(subheading_entry)\n",
    "\n",
    "        final_notes[\"headings\"].append(heading_entry)\n",
    "\n",
    "    return final_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cc64309-6606-45bd-8c7a-f6a3b3061b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_notes(notes):\n",
    "    for heading_data in notes[\"headings\"]:\n",
    "        heading_text = heading_data[\"text\"].strip()\n",
    "        print(f\"\\033[1m{heading_data['heading']}\\033[0m\")  # Bold heading\n",
    "        if heading_text:\n",
    "            print(f\"  {heading_text}\\n\")\n",
    "        \n",
    "        for subheading_data in heading_data[\"subheadings\"]:\n",
    "            subheading_text = subheading_data[\"text\"].strip()\n",
    "            print(f\"    \\033[94m{subheading_data['subheading']}\\033[0m\")  # Blue subheading\n",
    "            if subheading_text:\n",
    "                print(f\"      {subheading_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fabc2669-4718-4d27-ba89-a3a40c5e570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: D:/Studies Material/OLA Project/Slides.pdf\n",
      "Press ENTER to start recording. Press SPACE to move to the next slide. Press ESC to exit.\n",
      "Extracting text from Slide 1...\n",
      "Recording for Slide 0... Press SPACE to stop recording and move to the next slide.\n",
      "Recording stopped.\n",
      "Extracting text from Slide 2...\n",
      "Recording for Slide 1... Press SPACE to stop recording and move to the next slide.\n",
      "Recording stopped.\n",
      "Transcribing...\n",
      "\n",
      "\u001b[1m1. Process Management:\u001b[0m\n",
      "  Process management is a systematic approach to planning, organizing, and controlling the execution of tasks, projects, or activities to achieve a specific goal or objective. It involves the design, implementation, and monitoring of processes to ensure efficient and effective use of resources, minimize waste, and maximize productivity. The key principles of process management include process planning, process goals identification, and process areas for improvement.\n",
      "\n",
      "    \u001b[94m1.1. Process States:\u001b[0m\n",
      "      In conclusion, a process state is a fundamental concept in computer science that refers to the current status of a program or process in a system. It encompasses execution, memory, and input/output operations, and can be in one of five states: New, Ready, Running, Waiting, or Terminated. Understanding process states is essential for developing efficient, scalable, and reliable software systems.\n",
      "\n",
      "    \u001b[94m1.2. Scheduling Algorithms:\u001b[0m\n",
      "      In conclusion, scheduling algorithms like first-come-first-served, round-robin, and shortest job next are used to manage multiple processes efficiently. By understanding these concepts and algorithms, we can design and implement efficient scheduling algorithms that optimize performance and efficiency.\n",
      "\n",
      "\u001b[1m2. Memory Management:\u001b[0m\n",
      "  Memory management is a critical concept in computer science that involves allocating and deallocating memory resources in a computer system. The key principles of memory management include memory fragmentation, efficient memory management, and memory waste minimization. Effective memory management ensures efficient use of memory resources, reduces memory waste, and improves overall system performance.\n",
      "\n",
      "    \u001b[94m2.1. Paging:\u001b[0m\n",
      "      Paging is a memory management technique that allocates and deallocates memory resources to programs or processes. It's a crucial concept in operating systems, allowing programs to use more memory than is physically available. By allocating a fixed-size block of memory to a program or process, paging helps reduce fragmentation and improve system performance.\n",
      "\n",
      "    \u001b[94m2.2. Segmentation:\u001b[0m\n",
      "      Divides memory into variable-sized segments based on logical divisions in programs. Segments are organized in a hierarchical structure, with each segment representing a different level of abstraction. Two common techniques are paging and segmentation, which divide memory into fixed-size blocks and reduce fragmentation. By understanding these concepts, developers can create more efficient and effective memory management systems.\n",
      "\n",
      "    \u001b[94m2.3. Process Scheduling:\u001b[0m\n",
      "      Process scheduling is a critical concept in computer science that involves allocating system resources to processes (programs or threads) to maximize efficiency and resource utilization. The goal of process scheduling is to ensure that each process receives a fair share of processing time, minimizing delays, and maximizing throughput. Additionally, process scheduling determines the order in which processes execute on the CPU to maximize performance and efficiency.\n",
      "\n",
      "Lecture transcription completed successfully.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Runs the optimized pipeline for real-time lecture recording and slide processing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Global   variables for tracking slides and audio data\n",
    "    global current_slide, slide_texts, audio_data\n",
    "\n",
    "    # Extract slides from a selected PDF file\n",
    "    slides = extract_slides()\n",
    "\n",
    "    print(\"Press ENTER to start recording. Press SPACE to move to the next slide. Press ESC to exit.\")\n",
    "\n",
    "    # Wait for ENTER once to start recording the first slide\n",
    "    while not keyboard.is_pressed('enter'):\n",
    "        time.sleep(0.1)\n",
    "  \n",
    "    # Initialize lists for audio processing\n",
    "    audio_threads = []\n",
    "    audio_files = []\n",
    "\n",
    "    # Iterate through each slide for processing\n",
    "    while current_slide < len(slides):\n",
    "        stop_event = threading.Event()  # Event to signal recording stop\n",
    "        \n",
    "        # Extract text from the current slide and store it\n",
    "        slide_texts.append(extract_text_from_slide(slides[current_slide], current_slide + 1))\n",
    "\n",
    "        # Set up audio filename\n",
    "        audio_filename = f\"slide_{current_slide}.wav\"\n",
    "\n",
    "        # Reset audio data buffer to prevent memory buildup\n",
    "        audio_data = []\n",
    "\n",
    "        # Start a new thread to record audio\n",
    "        audio_thread = threading.Thread(target=record_audio, args=(16000, stop_event))\n",
    "        audio_thread.start()\n",
    "        audio_threads.append((audio_thread, stop_event))\n",
    "\n",
    "        print(f\"Recording for Slide {current_slide}... Press SPACE to stop recording and move to the next slide.\")\n",
    "\n",
    "        # Wait for SPACE or ESC key to proceed\n",
    "        while not stop_event.is_set():\n",
    "            if keyboard.is_pressed('space'):\n",
    "                stop_event.set()  # Stop recording when SPACE is pressed\n",
    "                break\n",
    "            if keyboard.is_pressed('esc'):\n",
    "                print(\"Exiting program...\")\n",
    "                stop_event.set()\n",
    "                return\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Ensure the audio recording thread finishes before processing\n",
    "        audio_thread.join()\n",
    "\n",
    "        # Preprocess and Save Recorded Audio\n",
    "        if audio_data:  # Ensure there is data before saving\n",
    "            audio_array = np.concatenate(audio_data, axis=0)  # Convert list of chunks to numpy array\n",
    "            \n",
    "            # Apply preprocessing (noise reduction, normalization)\n",
    "            processed_audio = preprocess_audio(audio_array)\n",
    "\n",
    "            # Save the preprocessed audio to disk\n",
    "            save_audio(audio_filename, processed_audio)\n",
    "            audio_files.append(audio_filename)\n",
    "\n",
    "        # Move to the next slide\n",
    "        current_slide += 1\n",
    "\n",
    "    # Wait for all audio recordings to finish processing\n",
    "    for thread, stop_event in audio_threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Batch Process Audio Using CUDA for Fastest Transcription\n",
    "    print(\"Transcribing...\\n\")\n",
    "\n",
    "    def transcribe_audio_wrapper(audio_filename):\n",
    "        \"\"\"\n",
    "        Helper function to transcribe audio using the Whisper model.\n",
    "        \"\"\"\n",
    "        file_path = rf\"D:\\Studies Material\\OLA Project\\audio_files\\{audio_filename}\"\n",
    "        return transcribe_audio(whisper_model, file_path)\n",
    "\n",
    "    # Use ThreadPoolExecutor to speed up transcription\n",
    "    with ThreadPoolExecutor(max_workers=min(torch.cuda.device_count(), 4)) as executor:\n",
    "        transcribed_texts = list(executor.map(transcribe_audio_wrapper, audio_files))\n",
    "  \n",
    "    # Combine extracted slide text with transcribed audio\n",
    "    for i, transcribed_text in enumerate(transcribed_texts):\n",
    "        combined_text = combine_slide_text_with_audio(slide_texts[i], transcribed_text, t5model, t5tokenizer)\n",
    "        notes = generate_notes(combined_text)\n",
    "        print_notes(notes)  # Display or store the final structured lecture notes\n",
    " \n",
    "    print(\"Lecture transcription completed successfully.\")\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":  \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
